---
title: "stat151a_cleaned"
author: "Kelly Trinh"
date: "12/11/2020"
output: html_document
---




Exploring data + cleaning data (EDA)
	Correlation
	Interaction
	Bivariate distribution
How we got the model (variable selection)
Diagnostics + Assessment
Interpretation + Application
Prediction
Bootstrap
Conclusion
drawbacks
	How to improve




```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
library(fivethirtyeight) #source of data
library(corrplot) #correlation plot
library(dplyr) #data frame manipulation
library(ggplot2) #plotting
library(leaps)#variable selection: adjr2, bic, cp
library(olsrr)#variable selection: aic, p-val
library(car) # Anova
library(fastDummies)
library(faraway)
library(caret)
```

### Introduction

Not all college students are informed about how the choice of their major can impact their earnings after graduation. We aim to provide an overview for college students of the financial standing of graduates from different majors categories. Our research question is: 

**Given a major category, what's likely to be my median income as a full-time, year-round worker?**

We will be building an OLS model to explain the association between major category and median income; and to predict the median income given a major category. 

#### Dataset 
The dataset we use is the recent-grad dataset from fivethirtyeight. 
https://www.rdocumentation.org/packages/fivethirtyeight/versions/0.6.1/topics/college_recent_grads

It contains income and other information about 16 different major categories.
Below is a detailed description of each variable: 

|Header            |	Description                                   |
|------------------|------------------------------------------------|
`Rank` | Rank by median earnings 
`Major_code` | Major code
`Major`	| Major description 
`Major_category`	| Category of major 
`Total` |	Total number of people with major
`Sample_size` |	Sample size of full-time, year-round
`Men`	| Male graduates
`Women`	| Female graduates
`ShareWomen` |	Women as share of total
`Employed` |	Number employed 
`Full_time`	| Employed 35 hours or more
`Part_time` |	Employed less than 35 hours
`Full_time_year_round` |	Employed at least 50 weeks and at least 35 hours 
`Unemployed` |	Number unemployed 
`Unemployment_rate` |	Unemployed / (Unemployed + Employed)
`Median` |	Median earnings of full-time, year-round workers
`P25th` |	25th percentile of earnings
`P75th` |	75th percentile of earnings
`College_jobs` |	Number with job requiring a college degree
`Non_college_jobs` |	Number with job not requiring a college degree
`Low_wage_jobs` |	Number in low-wage service jobs

### EDA and data cleaning 

Here are some modifications we did to our data:

1. The target variable `median` is right-skewed, so we log transformed it.

2. There is only one row with missing variables, so we omitted the row.

3. When examining the correlation plots, we see that many variables are highly correlated. For example, `college_jobs`, `employed_fulltime`, `employed_parttime` are highly correlated. We transforms some of these variables into `college_job_prop` and `full_time_yearround_prop`, which gives the proportions rather than an absolute number. We believe these variables would be less correlated with other explanatory variables. 

4. We standardized continuous variables to prepare for variable selection and transformed categorical variables into dummies.

5. We also performed a test-train split; the test set will be used to guard against overfitting and to build prediction intervals. 


```{r data_cleaning}
# load data
crg <- college_recent_grads
crg <- na.omit(crg)

# viewing correlations 
crg["log_median"] <- log(crg$median)
crg["college_job_prop"] <- crg$college_jobs / (crg$college_jobs + crg$non_college_jobs)
crg["full_time_yearround_prop"] <- crg$employed_fulltime_yearround / crg$employed
crg <- na.omit(crg)
cont <- select_if(crg, is.numeric)
cont <- na.omit(cont)
corrplot(cor(cont), type = "upper", order = "AOE")

# transform data
standardized <- data.frame(scale(cont))
major_category <- crg$major_category
crg <- cbind(standardized, major_category)

# temporary data frame to use later 
temp <- crg 

# transform categorical variable 
crg <- dummy_cols(crg, select_columns = "major_category", remove_selected_col = TRUE,remove_first_dummy = TRUE)
column_names <- make.names(names(crg),unique=TRUE)
colnames(crg) <- column_names

# test train split 
set.seed(11)
#randomly take 2:8 of data for training
training_size <- sample(dim(crg)[1],nrow(crg)*0.8)
training_data <- crg[training_size,]
testing_data <- crg[-training_size,]

```
We also created an overview of the distribution of median incomes from different major categories. 
```{r visualizations }

# major category versus median income 
ggplot(aes(x = major_category, y = log_median), data = temp) + geom_boxplot() + coord_flip()

```

### Model selection

#### Methodology 

We used backward and forward selection to filter out the existing variables; we consider interaction terms later. We produced six models: the 3 best models selected by backward selection in terms of Mallow's Cp, BIC, and adjusted R squared; and the 3 best models selected by forward selection using the same criterias. To decide between the 6 models, we performed leave-one-out cross validation (LOOCV) and calculated the root mean squared error (RMSE). We select the model with the lowest LOOCV RMSE, which is the model selected using backward selection with adjusted R squared as a criteria (this selected model is the same model chosen from backward selection using Mallow's Cp). The selected model without interaction terms has RMSE of `0.2435908`. We ensure to check for overfitting by training the selected model using the training set, predict on the testing set, and look at the sum of error squared and the correlation between the fitted values and the actual values. 

Next, we consider adding interaction terms. We used the Analysis of Variance test to determine which interaction terms to add to the model. To compare this model and the model without interaction terms, we again calculate the LOOCV RMSE for the former and compare with the RMSE of the latter. We found that the model with interaction terms perform better; specficially its LOOCV RMSE is `0.1933749`.

The two methods of variable selection (backward/forward versus Analysis of Variance) each have their weaknesses. Backward/forward selection add variable step-wise, so this method doesn't guarantee to select the best subset of explanatory variables; furthermore, it doesn't obey the principle of marginality and doesn't treat the variable `major_category` as a single category. Nevertheless, it doesn't rely on the assumption that the explanatory variable is normally distributed, while the Analysis of Variance test does. The Analysis of Variance test does obey the principle of marginality, and hence is better suited to select interaction terms. Due to these weaknesses, the model we selected didn't include all of the original 16 `major_category`, but only `Computers & Mathematics`, `Engineering`, and `Biology & Life Science`, which encompasses 31% of all the majors listed in our dataset. Our model selection method has made the `major_category` variables coarser. 


```{r prepare_data_for_selection}
unique(college_recent_grads$major_category)
nrow(college_recent_grads[college_recent_grads$major_category %in% c("Computers & Mathematics", "Biology & Life Science", "Engineering"), ])

nrow(college_recent_grads)
Y <- as.matrix(crg$log_median)

X <- as.matrix(subset(crg,select=-c(log_median,median, major_code, p25th, p75th)))
# Check for linear dependencies, remove "men" or "total"
alias(lm(Y~X))
X <- as.matrix(subset(crg,select=-c(log_median,median, major_code, p25th, p75th,  men)))
```

#### Variable selection without interactions: backward/forward/cross-validation 

```{r backward_forward_selection}

#Forward Selection
model <- regsubsets(x = X, y = Y, method = "forward", nbest = 1) %>% summary()

adjustr2 <- model$adjr2
BIC <- model$bic
mallow_Cp <- model$cp


cbind(model$which[which.max(adjustr2),], model$which[which.min(BIC),],model$which[which.min(mallow_Cp),])

#Backward Selection

model <- regsubsets(x = X, y = Y, method = "backward", nbest = 1) %>% summary()


adjustr2 <- model$adjr2
BIC <- model$bic
mallow_Cp <- model$cp
cbind(model$which[which.max(adjustr2),], model$which[which.min(BIC),],model$which[which.min(mallow_Cp),])

```

```{r backward_adjusted_r_squared}
# Backward Adjusted R-squared CV
tc <- trainControl(method = "LOOCV")
b_adjr2_mod_cv <- train(log_median ~ employed + employed_fulltime_yearround + 
                          college_job_prop +
                          major_category_Biology...Life.Science+
                          major_category_Computers...Mathematics + 
                          rank + sharewomen + major_category_Engineering , 
                        data = training_data, method = "lm", trControl = tc)
b_adjr2_mod_cv.rmse <- b_adjr2_mod_cv$results[,"RMSE"]

```


```{r backward_bic}
# Backward BIC CV
b_bic_mod_cv <- train(log_median ~ rank + college_job_prop +  major_category_Engineering, data = training_data, method = "lm", trControl = tc )
b_bic_mod_cv.rmse <- b_bic_mod_cv$results[, "RMSE"]
```

```{r backward_mallow_cp}
# Backward Mallow's Cp CV
b_cp_mod_cv <- train(log_median ~ employed + employed_fulltime_yearround + 
                       college_job_prop +  major_category_Biology...Life.Science
                     + major_category_Computers...Mathematics + rank +
                       major_category_Engineering + sharewomen , data = training_data , 
                     method = "lm", trControl = tc)
b_cp_mod_cv.rmse <- b_cp_mod_cv$results[, "RMSE"]
```


```{r forward_all_three_criterias}
# Forward Adjusted R-squared CV
f_adjr2_model_cv <- train(log_median ~ rank + unemployment_rate + 
                           college_job_prop + major_category_Arts + 
                           major_category_Biology...Life.Science  + 
                           major_category_Engineering  + 
                           major_category_Humanities...Liberal.Arts  + 
                           major_category_Industrial.Arts...Consumer.Services , 
                           data = training_data, method = "lm", trControl = tc)

f_adjr2_model_cv.rmse <- f_adjr2_model_cv$results[, "RMSE"]

# Forward BIC CV
f_BIC_model_cv <- train(log_median ~ rank + college_job_prop + 
                           major_category_Engineering, 
                       data = training_data, method = "lm", trControl = tc)

f_BIC_model_cv.rmse <- f_BIC_model_cv$results[, "RMSE"]

# Forward Mallow's Cp CV
f_Cp_model_cv <- train(log_median ~ rank + unemployment_rate + 
                          college_job_prop + major_category_Arts + 
                         major_category_Engineering + 
                         major_category_Humanities...Liberal.Arts, 
                      data = training_data, method = "lm", trControl = tc)

f_Cp_model_cv.rmse <- f_Cp_model_cv$results[, "RMSE"]
```

```{r check_overfitting}
#checking for overfitting

# model selected from backward/forward selection 
model.backfor <- lm(log_median ~ employed + employed_fulltime_yearround + 
                          college_job_prop +
                          major_category_Biology...Life.Science+
                          major_category_Computers...Mathematics + 
                          rank + sharewomen + major_category_Engineering, 
                    data = training_data)

model.backfor_predict <- predict(model.backfor, newdata = testing_data, type = "response")

# sum of error squared 
sum((testing_data$log_median - model.backfor_predict)^2)

# correlation between actual values and predicted values 
cor(model.backfor_predict, testing_data$log_median)**2

```


Summary table description: 

`forward.rmse` and `backward.rmse` are columns with the RMSE from prediction on the testing dataset depending on either forward selection or backward selection. 
Each of the row represent the criteria in which to choose the model from either forward or backward selection: `Adjusted R squared`, `BIC`, `Mallow's Cp`. 

```{r CV_rmse_summary}

criterias <- c("Adjusted R squared", "BIC", "Mallow's Cp")
forward.rmse <- c(f_adjr2_model_cv.rmse, f_BIC_model_cv.rmse, f_Cp_model_cv.rmse)
backward.rmse <- c(b_adjr2_mod_cv.rmse, b_bic_mod_cv.rmse, b_cp_mod_cv.rmse)

# table of RMSE of each model 
data.frame(criterias, forward.rmse, backward.rmse)
```

#### Variable selection with interactions 

To examine possible interaction terms, we look at coplots, created on the training data set with standardized continuous variables. The coplots show that `sharewomen` and `employed_fulltime_yearround` seem to have interactions with `major_category`. However, these coplots are inconclusive, so we will run an Analysis of Variance test to determine which interaction terms to keep. 

```{r coplots, warning=FALSE, message=FALSE}
#interactions

# temp dataframe for co-plots
temp.training <- temp[temp$rank %in% training_data$rank,]
temp.coplot <- temp.training[temp.training$major_category %in% 
                               c("Biology & Life Science",
                                 "Computers & Mathematics", 
                                 "Engineering"), ]


# share women and median income 
xyplot(log_median ~ sharewomen | major_category, data = temp.coplot ,
       panel = function(x, y) {
panel.xyplot(x, y, type = c("p", "smooth", "r"))
})

# rank and median income 
xyplot(log_median ~ rank | major_category, data = temp.coplot ,
       panel = function(x, y) {
panel.xyplot(x, y, type = c("p", "smooth", "r"))
})

# college job proportion and median income 
xyplot(log_median ~ college_job_prop | major_category, data = temp.coplot ,
       panel = function(x, y) {
panel.xyplot(x, y, type = c("p", "smooth", "r"))
})

# employed fulltime and median income 
xyplot(log_median ~ employed_fulltime_yearround | major_category, data = temp.coplot ,
       panel = function(x, y) {
panel.xyplot(x, y, type = c("p", "smooth", "r"))
})


```
We will keep all the original terms (not interaction terms) and will use the Analysis of Variance test to determine which interaction terms to add. This test, unlike the coplots shown above, show that interaction terms `rank:major_category_Engineering` and `sharewomen:major_category_Engineering` have significant p-values. We will include those variables in our final selected model. 

```{r full_model_interactions}
# model selected from backward/forward selection 
model.backfor <- lm(log_median ~ employed + employed_fulltime_yearround + 
                          college_job_prop +
                          major_category_Biology...Life.Science+
                          major_category_Computers...Mathematics + 
                          rank + sharewomen + major_category_Engineering, 
                    data = training_data)

# full model with all the interaction terms 
model.interacts <- lm(log_median ~ 
                        rank + sharewomen + 
                        employed + employed_fulltime_yearround + 
                        college_job_prop +
                        major_category_Biology...Life.Science+
                        major_category_Computers...Mathematics + 
                        major_category_Engineering +
                        major_category_Biology...Life.Science * rank+
                        major_category_Computers...Mathematics * rank+ 
                        major_category_Engineering * rank +
                        major_category_Biology...Life.Science * sharewomen+
                        major_category_Computers...Mathematics * sharewomen + 
                        major_category_Engineering * sharewomen +
                        major_category_Biology...Life.Science * employed_fulltime_yearround+
                        major_category_Computers...Mathematics * employed_fulltime_yearround+ 
                        major_category_Engineering * employed_fulltime_yearround+
                        major_category_Biology...Life.Science * college_job_prop+
                        major_category_Computers...Mathematics *college_job_prop + 
                        major_category_Engineering *college_job_prop, 
                        data = training_data)


# anova test
Anova(model.interacts)
```

```{r interactions}

# reduced model after running Anova test 
model.inter_reduced <- lm(log_median ~ 
                        rank + sharewomen + 
                        employed + employed_fulltime_yearround + 
                        college_job_prop +
                        major_category_Biology...Life.Science+
                        major_category_Computers...Mathematics + 
                        major_category_Engineering +
                        major_category_Engineering * rank +
                        major_category_Engineering * sharewomen,  
                        data = training_data)
   
anova(model.interacts, model.inter_reduced)
```

```{r compare_models}
# compare model with and without interactions in terms of LOOCV RMSE
interactions_cv <- train(log_median ~ 
                        rank + sharewomen + 
                        employed + employed_fulltime_yearround + 
                        college_job_prop +
                        major_category_Biology...Life.Science+
                        major_category_Computers...Mathematics + 
                        major_category_Engineering +
                        major_category_Engineering * rank +
                        major_category_Engineering * sharewomen, 
                      data = training_data, method = "lm", trControl = tc)

interactions_cv.rmse <- interactions_cv$results[, "RMSE"]

interactions_cv.rmse

```

#### Results

After considering the interaction terms, the selected model we chose includes the following variables:
`rank`,`sharewomen`,`employed`, `employed_fulltime_yearround`, `college_job_prop`, 
`major_category_Biology...Life.Science`, `major_category_Computers...Mathematics`, `major_category_Engineering`, `major_category_Engineering * rank`, and `major_category_Engineering * sharewomen`.

### Diagnostics 

We then perform diagnostics on outliers and assumptions. From the `Residuals vs Fitted` and `Scale-Location` plots, we see that there's no pattern in the studentized residuals against the fitted values, so we conclude that the response variable has a quite linear relationship with the explanatory variables, and the errors have constant residuals. Based on the `QQ plot`, we see that most of the data lines around the theoretical line well, meaning that the response variable is somewhat normally distributed. However, according to the `Residuals vs Leverage` plot, there are some outliers with higher Cook's distance, notably. Below, we fitted the model selected above without these points. We calculate the LOOCV RMSE of this new model. 

```{r diagnostics_plot}
plot(model.inter_reduced)
```
```{r outliers_diagnostics}
# outlier rows 
training_data[c(14, 33, 74, 87),]

# refit model with the outliers 
interactions_no_outliers <- train(log_median ~ 
                        rank + sharewomen + 
                        employed + employed_fulltime_yearround + 
                        college_job_prop +
                        major_category_Biology...Life.Science+
                        major_category_Computers...Mathematics + 
                        major_category_Engineering +
                        major_category_Engineering * rank +
                        major_category_Engineering * sharewomen, 
                      data = training_data[-c(14, 33, 74, 87),], method = "lm", trControl = tc)


# see CV RMSE
interactions_no_outliers.rmse <- interactions_no_outliers$results[, "RMSE"]
interactions_no_outliers.rmse
```

### Prediction


```{r predictions}

# final model selected 

model.final <- lm(log_median ~ 
                        rank + sharewomen + 
                        employed + employed_fulltime_yearround + 
                        college_job_prop +
                        major_category_Biology...Life.Science+
                        major_category_Computers...Mathematics + 
                        major_category_Engineering +
                        major_category_Engineering * rank +
                        major_category_Engineering * sharewomen, 
                      data = training_data[-c(14, 33, 74, 87),])
                  

# new data from testing set 
set.seed(11)
new_data1 <- testing_data[sample(nrow(testing_data), 1), ]


# reporting prediction intervals (standardized)
predict.standard <- predict(model.final, newdata = new_data1, interval = "predict")
predict.standard

# bring back to original scale 
predict.original <- exp(predict.standard * sd(log(college_recent_grads$median)) + mean(log(college_recent_grads$median)))

# get data from original data table
new_data1.rank <- new_data1[,1] * sd(college_recent_grads$rank) + mean(college_recent_grads$rank)
temp1 <- cbind(predict.original, college_recent_grads[ceiling(new_data1.rank), ]$median)
colnames(temp1) <- c(colnames(predict.standard), "actual")

# compare predicted data and actual value
temp1

```

### Reporting 


```{r Bootstrap}
#bootstrap for confidence interval

modeling <- lm(log_median ~
                        rank + sharewomen +
                        employed + employed_fulltime_yearround +
                        college_job_prop +
                        major_category_Biology...Life.Science+
                        major_category_Computers...Mathematics +
                        major_category_Engineering +
                        major_category_Engineering * rank +
                        major_category_Engineering * sharewomen,
                      data = training_data[-c(14, 33, 74, 87),])


#setting up nonparametric bootstrap
set.seed(11)
crg_data <- as.matrix(subset(crg,select=-c(median, major_code, p25th, p75th)))
sampling_size <- sample(dim(crg_data)[1],nrow(crg_data)*0.5)
sample_dist <- data.frame(crg_data[sampling_size,])

any(is.na(sample_dist))


any(is.na(sample_dist))

#bootstrap function
get_coef_resample <- function(){
   #makes a resample from sample dist
   resample <- sample_n(sample_dist, size = nrow(sample_dist),replace = TRUE)


   slopes <- lm(log_median ~
                        rank + sharewomen +
                        employed + employed_fulltime_yearround +
                        college_job_prop +
                        major_category_Biology...Life.Science+
                        major_category_Computers...Mathematics +
                        major_category_Engineering +
                        major_category_Engineering * rank +
                        major_category_Engineering * sharewomen,
                      data = resample) %>% coef()
   return(slopes)
}

nonpara_slopes <- t(replicate(1000,get_coef_resample()))
#nonpara_slopes[,8] #math coef has na values
as.data.frame(nonpara_slopes)
any(is.na(nonpara_slopes)) #checking if there're any NA
nonpara_slopes <- na.omit(nonpara_slopes) #removing na values
for(i in 1:ncol(nonpara_slopes)){hist(nonpara_slopes[,i])} #hist of each coef
summary(modeling)
nrow(nonpara_slopes)
col_CI = 1
CI <- nonpara_slopes[order(nonpara_slopes[,col_CI]),]
cbind(CI[25,col_CI],modeling$coefficients[col_CI],CI[975,col_CI])



nonpara_slopes <- t(replicate(1000,get_coef_resample()))
#nonpara_slopes[,8] #math coef has na values

as.data.frame(nonpara_slopes)

any(is.na(nonpara_slopes)) #checking if there're any NA
nonpara_slopes <- na.omit(nonpara_slopes) #removing na values


for(i in 1:ncol(nonpara_slopes)){hist(nonpara_slopes[,i])} #hist of each coef
summary(modeling)

nrow(nonpara_slopes)


col_CI = 1
CI <- nonpara_slopes[order(nonpara_slopes[,col_CI]),]
cbind(CI[25,col_CI],modeling$coefficients[col_CI],CI[975,col_CI])


#checking if model is in 95% CI
is_in_CI <- NULL
for(i in 1:ncol(nonpara_slopes)){
   CI <- nonpara_slopes[order(nonpara_slopes[,i]),] #orders data based on ith col
   
   print(CI[25,i],CI[975,i])


   if(CI[25,i] <= modeling$coefficients[i] & modeling$coefficients[i] <= CI[975,i]){
      is_in_CI <- c(TRUE,is_in_CI)
   } else {
      is_in_CI <- c(FALSE,is_in_CI)
   }
}

```




